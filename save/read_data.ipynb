{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import bert_utils\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import time\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, XLNetTokenizer, XLNetModel \n",
    "pretrained_model = BertModel.from_pretrained('bert-base-chinese')\n",
    "# https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin\n",
    "tokenizer =BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "#model_bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BertForSequenceClassification in module pytorch_transformers.modeling_bert:\n",
      "\n",
      "class BertForSequenceClassification(BertPreTrainedModel)\n",
      " |  BertForSequenceClassification(config)\n",
      " |  \n",
      " |  Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of\n",
      " |  the pooled output) e.g. for GLUE tasks.     The BERT model was proposed in\n",
      " |  `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_\n",
      " |  by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It's a bidirectional transformer\n",
      " |  pre-trained using a combination of masked language modeling objective and next sentence prediction\n",
      " |  on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n",
      " |  \n",
      " |  This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\n",
      " |  refer to the PyTorch documentation for all matter related to general usage and behavior.\n",
      " |  \n",
      " |  .. _`BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`:\n",
      " |      https://arxiv.org/abs/1810.04805\n",
      " |  \n",
      " |  .. _`torch.nn.Module`:\n",
      " |      https://pytorch.org/docs/stable/nn.html#module\n",
      " |  \n",
      " |  Parameters:\n",
      " |      config (:class:`~pytorch_transformers.BertConfig`): Model configuration class with all the parameters of the model.\n",
      " |  \n",
      " |  Inputs:\n",
      " |      **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n",
      " |          Indices of input sequence tokens in the vocabulary.\n",
      " |          To match pre-training, BERT input sequence should be formatted with [CLS] and [SEP] tokens as follows:\n",
      " |  \n",
      " |          (a) For sequence pairs:\n",
      " |  \n",
      " |              ``tokens:         [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]``\n",
      " |              \n",
      " |              ``token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1``\n",
      " |  \n",
      " |          (b) For single sequences:\n",
      " |  \n",
      " |              ``tokens:         [CLS] the dog is hairy . [SEP]``\n",
      " |              \n",
      " |              ``token_type_ids:   0   0   0   0  0     0   0``\n",
      " |  \n",
      " |          Indices can be obtained using :class:`pytorch_transformers.BertTokenizer`.\n",
      " |          See :func:`pytorch_transformers.PreTrainedTokenizer.encode` and\n",
      " |          :func:`pytorch_transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n",
      " |      **position_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n",
      " |          Indices of positions of each input sequence tokens in the position embeddings.\n",
      " |          Selected in the range ``[0, config.max_position_embeddings - 1]``.\n",
      " |      **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n",
      " |          Segment token indices to indicate first and second portions of the inputs.\n",
      " |          Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``\n",
      " |          corresponds to a `sentence B` token\n",
      " |          (see `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_ for more details).\n",
      " |      **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\n",
      " |          Mask to avoid performing attention on padding token indices.\n",
      " |          Mask values selected in ``[0, 1]``:\n",
      " |          ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
      " |      **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n",
      " |          Mask to nullify selected heads of the self-attention modules.\n",
      " |          Mask values selected in ``[0, 1]``:\n",
      " |          ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n",
      " |  \n",
      " |      **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
      " |          Labels for computing the sequence classification/regression loss.\n",
      " |          Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
      " |          If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
      " |          If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
      " |  \n",
      " |  Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
      " |      **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
      " |          Classification (or regression if config.num_labels==1) loss.\n",
      " |      **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
      " |          Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
      " |      **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
      " |          list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
      " |          of shape ``(batch_size, sequence_length, hidden_size)``:\n",
      " |          Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
      " |      **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
      " |          list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
      " |          Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      " |      model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
      " |      input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
      " |      labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
      " |      outputs = model(input_ids, labels=labels)\n",
      " |      loss, logits = outputs[:2]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BertForSequenceClassification\n",
      " |      BertPreTrainedModel\n",
      " |      pytorch_transformers.modeling_utils.PreTrainedModel\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, config)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, position_ids=None, head_mask=None)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BertPreTrainedModel:\n",
      " |  \n",
      " |  init_weights(self, module)\n",
      " |      Initialize the weights.\n",
      " |  \n",
      " |  load_tf_weights = load_tf_weights_in_bert(model, config, tf_checkpoint_path)\n",
      " |      Load tf checkpoints in a pytorch model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from BertPreTrainedModel:\n",
      " |  \n",
      " |  base_model_prefix = 'bert'\n",
      " |  \n",
      " |  config_class = <class 'pytorch_transformers.modeling_bert.BertConfig'>\n",
      " |      :class:`~pytorch_transformers.BertConfig` is the configuration class to store the configuration of a\n",
      " |      `BertModel`.\n",
      " |      \n",
      " |      \n",
      " |      Arguments:\n",
      " |          vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n",
      " |          hidden_size: Size of the encoder layers and the pooler layer.\n",
      " |          num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
      " |          num_attention_heads: Number of attention heads for each attention layer in\n",
      " |              the Transformer encoder.\n",
      " |          intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
      " |              layer in the Transformer encoder.\n",
      " |          hidden_act: The non-linear activation function (function or string) in the\n",
      " |              encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
      " |          hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
      " |              layers in the embeddings, encoder, and pooler.\n",
      " |          attention_probs_dropout_prob: The dropout ratio for the attention\n",
      " |              probabilities.\n",
      " |          max_position_embeddings: The maximum sequence length that this model might\n",
      " |              ever be used with. Typically set this to something large just in case\n",
      " |              (e.g., 512 or 1024 or 2048).\n",
      " |          type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
      " |              `BertModel`.\n",
      " |          initializer_range: The sttdev of the truncated_normal_initializer for\n",
      " |              initializing all weight matrices.\n",
      " |          layer_norm_eps: The epsilon used by LayerNorm.\n",
      " |  \n",
      " |  pretrained_model_archive_map = {'bert-base-cased': 'https://s3.amazona...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_transformers.modeling_utils.PreTrainedModel:\n",
      " |  \n",
      " |  prune_heads(self, heads_to_prune)\n",
      " |      Prunes heads of the base model.\n",
      " |      \n",
      " |      Arguments:\n",
      " |      \n",
      " |          heads_to_prune: dict with keys being selected layer indices (`int`) and associated values being the list of heads to prune in said layer (list of `int`).\n",
      " |  \n",
      " |  resize_token_embeddings(self, new_num_tokens=None)\n",
      " |      Resize input token embeddings matrix of the model if new_num_tokens != config.vocab_size.\n",
      " |      Take care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.\n",
      " |      \n",
      " |      Arguments:\n",
      " |      \n",
      " |          new_num_tokens: (`optional`) int:\n",
      " |              New number of tokens in the embedding matrix. Increasing the size will add newly initialized vectors at the end. Reducing the size will remove vectors from the end. \n",
      " |              If not provided or None: does nothing and just returns a pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\n",
      " |      \n",
      " |      Return: ``torch.nn.Embeddings``\n",
      " |          Pointer to the input tokens Embeddings Module of the model\n",
      " |  \n",
      " |  save_pretrained(self, save_directory)\n",
      " |      Save a model and its configuration file to a directory, so that it\n",
      " |      can be re-loaded using the `:func:`~pytorch_transformers.PreTrainedModel.from_pretrained`` class method.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pytorch_transformers.modeling_utils.PreTrainedModel:\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs) from builtins.type\n",
      " |      Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
      " |      \n",
      " |      The model is set in evaluation mode by default using ``model.eval()`` (Dropout modules are deactivated)\n",
      " |      To train the model, you should first set it back in training mode with ``model.train()``\n",
      " |      \n",
      " |      The warning ``Weights from XXX not initialized from pretrained model`` means that the weights of XXX do not come pre-trained with the rest of the model.\n",
      " |      It is up to you to train those weights with a downstream fine-tuning task.\n",
      " |      \n",
      " |      The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used by YYY, therefore those weights are discarded.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path: either:\n",
      " |      \n",
      " |              - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.\n",
      " |              - a path to a `directory` containing model weights saved using :func:`~pytorch_transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/``.\n",
      " |              - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
      " |      \n",
      " |          model_args: (`optional`) Sequence of positional arguments:\n",
      " |              All remaning positional arguments will be passed to the underlying model's ``__init__`` method\n",
      " |      \n",
      " |          config: (`optional`) instance of a class derived from :class:`~pytorch_transformers.PretrainedConfig`:\n",
      " |              Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n",
      " |      \n",
      " |              - the model is a model provided by the library (loaded with the ``shortcut-name`` string of a pretrained model), or\n",
      " |              - the model was saved using :func:`~pytorch_transformers.PreTrainedModel.save_pretrained` and is reloaded by suppling the save directory.\n",
      " |              - the model is loaded by suppling a local directory as ``pretrained_model_name_or_path`` and a configuration JSON file named `config.json` is found in the directory.\n",
      " |      \n",
      " |          state_dict: (`optional`) dict:\n",
      " |              an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.\n",
      " |              This option can be used if you want to create a model from a pretrained configuration but load your own weights.\n",
      " |              In this case though, you should check if using :func:`~pytorch_transformers.PreTrainedModel.save_pretrained` and :func:`~pytorch_transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n",
      " |      \n",
      " |          cache_dir: (`optional`) string:\n",
      " |              Path to a directory in which a downloaded pre-trained model\n",
      " |              configuration should be cached if the standard cache should not be used.\n",
      " |      \n",
      " |          output_loading_info: (`optional`) boolean:\n",
      " |              Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.\n",
      " |      \n",
      " |          kwargs: (`optional`) Remaining dictionary of keyword arguments:\n",
      " |              Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ``output_attention=True``). Behave differently depending on whether a `config` is provided or automatically loaded:\n",
      " |      \n",
      " |              - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the underlying model's ``__init__`` method (we assume all relevant updates to the configuration have already been done)\n",
      " |              - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class initialization function (:func:`~pytorch_transformers.PretrainedConfig.from_pretrained`). Each key of ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's ``__init__`` function.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          model = BertModel.from_pretrained('bert-base-uncased')    # Download model and configuration from S3 and cache.\n",
      " |          model = BertModel.from_pretrained('./test/saved_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n",
      " |          model = BertModel.from_pretrained('bert-base-uncased', output_attention=True)  # Update configuration during loading\n",
      " |          assert model.config.output_attention == True\n",
      " |          # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n",
      " |          config = BertConfig.from_json_file('./tf_model/my_tf_model_config.json')\n",
      " |          model = BertModel.from_pretrained('./tf_model/my_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`torch-nn-init`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.data.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  buffers(self, recurse=True)\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf.data), buf.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should reimplement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all floating point parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix='', recurse=True)\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix='', recurse=True)\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse=True)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |      \n",
      " |      .. warning ::\n",
      " |      \n",
      " |          The current implementation will not have the presented behavior\n",
      " |          for complex :class:`Module` that perform many operations.\n",
      " |          In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only\n",
      " |          contain the gradients for a subset of the inputs and outputs.\n",
      " |          For such :class:`Module`, you should use :func:`torch.Tensor.register_hook`\n",
      " |          directly on a specific input or output to get the required gradients.\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  requires_grad_(self, requires_grad=True)\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point desired :attr:`dtype` s. In addition, this method will\n",
      " |      only cast the floating point parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point type of\n",
      " |              the floating point parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pytorch_transformers.modeling_bert import BertForSequenceClassification, BertConfig\n",
    "help(BertForSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "ids = np.load('kfold/test_ids.npy')\n",
    "results =[]\n",
    "datanames = os.listdir('kfold/')\n",
    "for dataname in datanames:\n",
    "    if 'LSTM' in dataname:\n",
    "        results.append(np.load('kfold/'+dataname))\n",
    "print(len(results))\n",
    "average = np.average(results,axis=0)\n",
    "preds = np.argmax(average,axis=1) \n",
    "dic =[-1,0,1]\n",
    "result = [dic[i] for i in preds]\n",
    "test=pd.DataFrame(result,index=ids,columns=['y'])\n",
    "test.to_csv('kfold/result_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_label_path = \"data/train_label.csv\"\n",
    "train_unlabel_path = \"data/train_unlabel.csv\"\n",
    "test_path =\"data/test.csv\"\n",
    "def get_data():\n",
    "    train_data = pd.read_csv(train_label_path)\n",
    "    #print('train_origin shape=',train_data.shape)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    train_data = train_data[train_data['情感倾向'].isin(['-1','0','1'])]  #去除标签错误的\n",
    "    labels = np.asarray(train_data['情感倾向'].astype(int)+1)\n",
    "    #print('train_shape=',train_data.shape)\n",
    "    return train_data['微博中文内容'].values,labels,test_data['微博中文内容'].values,test_data['微博id'].values\n",
    "train,labels,test,ids = get_data()\n",
    "unlabel_data = pd.read_csv(train_unlabel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"do_sample\": false,\n",
       "  \"eos_token_ids\": null,\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"is_decoder\": false,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"length_penalty\": 1.0,\n",
       "  \"max_length\": 20,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_beams\": 1,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_labels\": 2,\n",
       "  \"num_return_sequences\": 1,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": null,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"pruned_heads\": {},\n",
       "  \"repetition_penalty\": 1.0,\n",
       "  \"temperature\": 1.0,\n",
       "  \"top_k\": 50,\n",
       "  \"top_p\": 1.0,\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bfloat16\": false,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#help(nn.LSTM)\n",
    "pretrained_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module heapq:\n",
      "\n",
      "NAME\n",
      "    heapq - Heap queue algorithm (a.k.a. priority queue).\n",
      "\n",
      "MODULE REFERENCE\n",
      "    https://docs.python.org/3.7/library/heapq\n",
      "    \n",
      "    The following documentation is automatically generated from the Python\n",
      "    source files.  It may be incomplete, incorrect or include features that\n",
      "    are considered implementation detail and may vary between Python\n",
      "    implementations.  When in doubt, consult the module reference at the\n",
      "    location listed above.\n",
      "\n",
      "DESCRIPTION\n",
      "    Heaps are arrays for which a[k] <= a[2*k+1] and a[k] <= a[2*k+2] for\n",
      "    all k, counting elements from 0.  For the sake of comparison,\n",
      "    non-existing elements are considered to be infinite.  The interesting\n",
      "    property of a heap is that a[0] is always its smallest element.\n",
      "    \n",
      "    Usage:\n",
      "    \n",
      "    heap = []            # creates an empty heap\n",
      "    heappush(heap, item) # pushes a new item on the heap\n",
      "    item = heappop(heap) # pops the smallest item from the heap\n",
      "    item = heap[0]       # smallest item on the heap without popping it\n",
      "    heapify(x)           # transforms list into a heap, in-place, in linear time\n",
      "    item = heapreplace(heap, item) # pops and returns smallest item, and adds\n",
      "                                   # new item; the heap size is unchanged\n",
      "    \n",
      "    Our API differs from textbook heap algorithms as follows:\n",
      "    \n",
      "    - We use 0-based indexing.  This makes the relationship between the\n",
      "      index for a node and the indexes for its children slightly less\n",
      "      obvious, but is more suitable since Python uses 0-based indexing.\n",
      "    \n",
      "    - Our heappop() method returns the smallest item, not the largest.\n",
      "    \n",
      "    These two make it possible to view the heap as a regular Python list\n",
      "    without surprises: heap[0] is the smallest item, and heap.sort()\n",
      "    maintains the heap invariant!\n",
      "\n",
      "FUNCTIONS\n",
      "    heapify(...)\n",
      "        Transform list into a heap, in-place, in O(len(heap)) time.\n",
      "    \n",
      "    heappop(...)\n",
      "        Pop the smallest item off the heap, maintaining the heap invariant.\n",
      "    \n",
      "    heappush(...)\n",
      "        heappush(heap, item) -> None. Push item onto heap, maintaining the heap invariant.\n",
      "    \n",
      "    heappushpop(...)\n",
      "        heappushpop(heap, item) -> value. Push item on the heap, then pop and return the smallest item\n",
      "        from the heap. The combined action runs more efficiently than\n",
      "        heappush() followed by a separate call to heappop().\n",
      "    \n",
      "    heapreplace(...)\n",
      "        heapreplace(heap, item) -> value. Pop and return the current smallest value, and add the new item.\n",
      "        \n",
      "        This is more efficient than heappop() followed by heappush(), and can be\n",
      "        more appropriate when using a fixed-size heap.  Note that the value\n",
      "        returned may be larger than item!  That constrains reasonable uses of\n",
      "        this routine unless written as part of a conditional replacement:\n",
      "        \n",
      "            if item > heap[0]:\n",
      "                item = heapreplace(heap, item)\n",
      "    \n",
      "    merge(*iterables, key=None, reverse=False)\n",
      "        Merge multiple sorted inputs into a single sorted output.\n",
      "        \n",
      "        Similar to sorted(itertools.chain(*iterables)) but returns a generator,\n",
      "        does not pull the data into memory all at once, and assumes that each of\n",
      "        the input streams is already sorted (smallest to largest).\n",
      "        \n",
      "        >>> list(merge([1,3,5,7], [0,2,4,8], [5,10,15,20], [], [25]))\n",
      "        [0, 1, 2, 3, 4, 5, 5, 7, 8, 10, 15, 20, 25]\n",
      "        \n",
      "        If *key* is not None, applies a key function to each element to determine\n",
      "        its sort order.\n",
      "        \n",
      "        >>> list(merge(['dog', 'horse'], ['cat', 'fish', 'kangaroo'], key=len))\n",
      "        ['dog', 'cat', 'fish', 'horse', 'kangaroo']\n",
      "    \n",
      "    nlargest(n, iterable, key=None)\n",
      "        Find the n largest elements in a dataset.\n",
      "        \n",
      "        Equivalent to:  sorted(iterable, key=key, reverse=True)[:n]\n",
      "    \n",
      "    nsmallest(n, iterable, key=None)\n",
      "        Find the n smallest elements in a dataset.\n",
      "        \n",
      "        Equivalent to:  sorted(iterable, key=key)[:n]\n",
      "\n",
      "DATA\n",
      "    __about__ = 'Heap queues\\n\\n[explanation by François Pinard]\\n\\nH... t...\n",
      "    __all__ = ['heappush', 'heappop', 'heapify', 'heapreplace', 'merge', '...\n",
      "\n",
      "FILE\n",
      "    /home/zht/anaconda3/envs/pytorch/lib/python3.7/heapq.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "help(heapq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "微博id,微博发布时间,发布人账号,微博中文内容,微博图片,微博视频,情感倾向\n",
      "Finish read File len:99317,unlabel:86 no_text：597\n",
      "微博id,微博发布时间,发布人账号,微博中文内容,微博图片,微博视频\n",
      "Finish read File len:10000,unlabel:0 no_text：64\n",
      "train_data:89385,valid_data:9932,test_data:10000\n"
     ]
    }
   ],
   "source": [
    "xtrain,xvalid,ytrain,yvalid,test_data,ids = bert_utils.get_Bert_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_save_file(path,write_path):\n",
    "    with open(path,'r',encoding='gbk',errors='ignore') as f:\n",
    "        with open(write_path,'w',encoding='utf-8') as wf:\n",
    "            for line in f:\n",
    "                wf.write(line)\n",
    "re_save_file(train_label_path,'data/train_label.csv')\n",
    "re_save_file(train_unlabel_path,'data/train_unlabel.csv')\n",
    "re_save_file(test_path,'data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "sns.set(font_scale=2)\n",
    "train_data = pd.read_csv('data/train_label.csv')\n",
    "test_data =pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 99566 entries, 0 to 99999\n",
      "Data columns (total 7 columns):\n",
      "微博id      99566 non-null int64\n",
      "微博发布时间    99566 non-null object\n",
      "发布人账号     99566 non-null object\n",
      "微博中文内容    99566 non-null object\n",
      "微博图片      99566 non-null object\n",
      "微博视频      99566 non-null object\n",
      "情感倾向      99566 non-null object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 6.1+ MB\n",
      "None\n",
      "0     57287\n",
      "1     25374\n",
      "-1    16899\n",
      "-         1\n",
      "·         1\n",
      "9         1\n",
      "10        1\n",
      "4         1\n",
      "-2        1\n",
      "Name: 情感倾向, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 99560 entries, 0 to 99999\n",
      "Data columns (total 7 columns):\n",
      "微博id      99560 non-null int64\n",
      "微博发布时间    99560 non-null object\n",
      "发布人账号     99560 non-null object\n",
      "微博中文内容    99560 non-null object\n",
      "微博图片      99560 non-null object\n",
      "微博视频      99560 non-null object\n",
      "情感倾向      99560 non-null object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 6.1+ MB\n",
      "None\n",
      "0     57287\n",
      "1     25374\n",
      "-1    16899\n",
      "Name: 情感倾向, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna(axis=0,subset=['微博中文内容','情感倾向'])\n",
    "print(train_data.info())\n",
    "print(train_data['情感倾向'].value_counts())\n",
    "#train_data = train_data.loc[train_data['情感倾向']=='0']\n",
    "train_data = train_data[train_data['情感倾向'].isin(['-1','0','1'])]\n",
    "print(train_data.info())\n",
    "print(train_data['情感倾向'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A    B\n",
      "1  10  NaN\n",
      "2   4  5.0\n",
      "3   7  8.0\n",
      "    A  B\n",
      "1  10   \n",
      "2   4  5\n",
      "3   7  8\n",
      "    A    B\n",
      "1  10  NaN\n",
      "2   4  5.0\n",
      "3   7  8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zht/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([10,  4,  7])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([[2], [4, 5], [7, 8]],\n",
    "    index=['1', '2', '3'],\n",
    "     columns=['A', 'B'])\n",
    "df['A']['1'] =10\n",
    "print(df)\n",
    "print(df.fillna(''))\n",
    "print(df)\n",
    "np.array(df['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 11 columns):\n",
      "微博id       100000 non-null int64\n",
      "微博发布时间     100000 non-null object\n",
      "发布人账号      100000 non-null object\n",
      "微博中文内容     99646 non-null object\n",
      "微博图片       100000 non-null object\n",
      "微博视频       100000 non-null object\n",
      "情感倾向       99919 non-null object\n",
      "time       100000 non-null datetime64[ns]\n",
      "day_num    100000 non-null int64\n",
      "hour       100000 non-null int64\n",
      "len        100000 non-null int64\n",
      "dtypes: datetime64[ns](1), int64(4), object(6)\n",
      "memory usage: 8.4+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 6 columns):\n",
      "微博id      10000 non-null int64\n",
      "微博发布时间    10000 non-null object\n",
      "发布人账号     10000 non-null object\n",
      "微博中文内容    9963 non-null object\n",
      "微博图片      10000 non-null object\n",
      "微博视频      10000 non-null object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 468.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>微博id</th>\n",
       "      <th>微博发布时间</th>\n",
       "      <th>发布人账号</th>\n",
       "      <th>微博中文内容</th>\n",
       "      <th>微博图片</th>\n",
       "      <th>微博视频</th>\n",
       "      <th>情感倾向</th>\n",
       "      <th>time</th>\n",
       "      <th>day_num</th>\n",
       "      <th>hour</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4456072029125500</td>\n",
       "      <td>01月01日 23:50</td>\n",
       "      <td>存曦1988</td>\n",
       "      <td>写在年末冬初孩子流感的第五天，我们仍然没有忘记热情拥抱这2020年的第一天。带着一丝迷信，早...</td>\n",
       "      <td>['https://ww2.sinaimg.cn/orj360/005VnA1zly1gah...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1900-01-01 23:50:00</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4456074167480980</td>\n",
       "      <td>01月01日 23:58</td>\n",
       "      <td>LunaKrys</td>\n",
       "      <td>开年大模型…累到以为自己发烧了腰疼膝盖疼腿疼胳膊疼脖子疼#Luna的Krystallife#?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>1900-01-01 23:58:00</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4456054253264520</td>\n",
       "      <td>01月01日 22:39</td>\n",
       "      <td>小王爷学辩论o_O</td>\n",
       "      <td>偳癯空饩褪俏业，爹，发烧快好，毕竟美好的假期拿来养病不太好，假期还是要好好享受快乐，爹，新年...</td>\n",
       "      <td>['https://ww2.sinaimg.cn/thumb150/006ymYXKgy1g...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1900-01-01 22:39:00</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4456061509126470</td>\n",
       "      <td>01月01日 23:08</td>\n",
       "      <td>芩鎟</td>\n",
       "      <td>新年的第一天感冒又发烧的也太衰了但是我要想着明天一定会好的?</td>\n",
       "      <td>['https://ww2.sinaimg.cn/orj360/005FL9LZgy1gah...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1900-01-01 23:08:00</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4455979322528190</td>\n",
       "      <td>01月01日 17:42</td>\n",
       "      <td>changlwj</td>\n",
       "      <td>问：我们意念里有坏的想法了，天神就会给记下来，那如果有好的想法也会被记下来吗？答：那当然了。...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1900-01-01 17:42:00</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               微博id        微博发布时间      发布人账号  \\\n",
       "0  4456072029125500  01月01日 23:50     存曦1988   \n",
       "1  4456074167480980  01月01日 23:58   LunaKrys   \n",
       "2  4456054253264520  01月01日 22:39  小王爷学辩论o_O   \n",
       "3  4456061509126470  01月01日 23:08         芩鎟   \n",
       "4  4455979322528190  01月01日 17:42   changlwj   \n",
       "\n",
       "                                              微博中文内容  \\\n",
       "0  写在年末冬初孩子流感的第五天，我们仍然没有忘记热情拥抱这2020年的第一天。带着一丝迷信，早...   \n",
       "1    开年大模型…累到以为自己发烧了腰疼膝盖疼腿疼胳膊疼脖子疼#Luna的Krystallife#?   \n",
       "2  偳癯空饩褪俏业，爹，发烧快好，毕竟美好的假期拿来养病不太好，假期还是要好好享受快乐，爹，新年...   \n",
       "3                     新年的第一天感冒又发烧的也太衰了但是我要想着明天一定会好的?   \n",
       "4  问：我们意念里有坏的想法了，天神就会给记下来，那如果有好的想法也会被记下来吗？答：那当然了。...   \n",
       "\n",
       "                                                微博图片 微博视频 情感倾向  \\\n",
       "0  ['https://ww2.sinaimg.cn/orj360/005VnA1zly1gah...   []    0   \n",
       "1                                                 []   []   -1   \n",
       "2  ['https://ww2.sinaimg.cn/thumb150/006ymYXKgy1g...   []    1   \n",
       "3  ['https://ww2.sinaimg.cn/orj360/005FL9LZgy1gah...   []    1   \n",
       "4                                                 []   []    1   \n",
       "\n",
       "                 time  day_num  hour  len  \n",
       "0 1900-01-01 23:50:00        1    23  147  \n",
       "1 1900-01-01 23:58:00        1    23   47  \n",
       "2 1900-01-01 22:39:00        1    22   98  \n",
       "3 1900-01-01 23:08:00        1    23   30  \n",
       "4 1900-01-01 17:42:00        1    17  145  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.info()\n",
    "test_data.info()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data['微博中文内容'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     57619\n",
       "1     25392\n",
       "-1    16902\n",
       "-         1\n",
       "·         1\n",
       "9         1\n",
       "10        1\n",
       "4         1\n",
       "-2        1\n",
       "Name: 情感倾向, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['情感倾向'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['time'] = pd.to_datetime(train_data['微博发布时间'],format='%m月%d日  %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    51805\n",
       "1    48195\n",
       "Name: time, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['time'].dt.month.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9039000250>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['day_num'] = (train_data['time'].dt.month -1) *31 + train_data['time'].dt.day\n",
    "train_data['day_num'].value_counts(sort=False) \n",
    "train_data['time'].dt.hour.value_counts()\n",
    "train_data['hour'] = train_data['time'].dt.hour\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "sns.kdeplot(train_data.loc[train_data['情感倾向']=='0','hour'],ax=ax,label='sent(0)')\n",
    "sns.kdeplot(train_data.loc[train_data['情感倾向']=='1','hour'],ax=ax,label='sent(1)')\n",
    "sns.kdeplot(train_data.loc[train_data['情感倾向']=='-1','hour'],ax=ax,label='sent(-1)')\n",
    "\n",
    "#print(train_data.loc[train_data['情感倾向']=='0','hour'].value_counts())\n",
    "#print(train_data.loc[train_data['情感倾向']=='1','hour'].value_counts())\n",
    "#print(train_data.loc[train_data['情感倾向']=='-1','hour'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30819\n",
      "56451\n",
      "93473\n",
      "99975\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "train_data['len'] = train_data['微博中文内容'].astype(str).apply(len)\n",
    "print(len(train_data.loc[train_data['len']<=50]))\n",
    "print(len(train_data.loc[train_data['len']<=100]))\n",
    "print(len(train_data.loc[train_data['len']<=150]))\n",
    "print(len(train_data.loc[train_data['len']<=200]))\n",
    "print(len(train_data.loc[train_data['len']>200]))\n",
    "#type(train_data['len'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StratifiedKFold in module sklearn.model_selection._split:\n",
      "\n",
      "class StratifiedKFold(_BaseKFold)\n",
      " |  StratifiedKFold(n_splits='warn', shuffle=False, random_state=None)\n",
      " |  \n",
      " |  Stratified K-Folds cross-validator\n",
      " |  \n",
      " |  Provides train/test indices to split data in train/test sets.\n",
      " |  \n",
      " |  This cross-validation object is a variation of KFold that returns\n",
      " |  stratified folds. The folds are made by preserving the percentage of\n",
      " |  samples for each class.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=3\n",
      " |      Number of folds. Must be at least 2.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |          ``n_splits`` default value will change from 3 to 5 in v0.22.\n",
      " |  \n",
      " |  shuffle : boolean, optional\n",
      " |      Whether to shuffle each class's samples before splitting into batches.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default=None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`. Used when ``shuffle`` == True.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.model_selection import StratifiedKFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([0, 0, 1, 1])\n",
      " |  >>> skf = StratifiedKFold(n_splits=2)\n",
      " |  >>> skf.get_n_splits(X, y)\n",
      " |  2\n",
      " |  >>> print(skf)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |  StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for train_index, test_index in skf.split(X, y):\n",
      " |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      " |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      " |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      " |  TRAIN: [1 3] TEST: [0 2]\n",
      " |  TRAIN: [0 2] TEST: [1 3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Train and test sizes may be different in each fold, with a difference of at\n",
      " |  most ``n_classes``.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StratifiedKFold\n",
      " |      _BaseKFold\n",
      " |      BaseCrossValidator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_splits='warn', shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  split(self, X, y, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |          Note that providing ``y`` is sufficient to generate the splits and\n",
      " |          hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
      " |          ``X`` instead of actual training data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target variable for supervised learning problems.\n",
      " |          Stratification is done based on the y labels.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |      \n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Randomized CV splitters may return different results for each call of\n",
      " |      split. You can make the results identical by setting ``random_state``\n",
      " |      to an integer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |  \n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "help(StratifiedKFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LogisticRegression in module sklearn.linear_model.logistic:\n",
      "\n",
      "class LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      " |  LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='warn', max_iter=100, multi_class='warn', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |  \n",
      " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      " |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      " |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      " |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      " |  'sag', 'saga' and 'newton-cg' solvers.)\n",
      " |  \n",
      " |  This class implements regularized logistic regression using the\n",
      " |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      " |  that regularization is applied by default**. It can handle both dense\n",
      " |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      " |  floats for optimal performance; any other input format will be converted\n",
      " |  (and copied).\n",
      " |  \n",
      " |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      " |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      " |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      " |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      " |  'saga' solver.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : str, 'l1', 'l2', 'elasticnet' or 'none', optional (default='l2')\n",
      " |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      " |      'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
      " |      only supported by the 'saga' solver. If 'none' (not supported by the\n",
      " |      liblinear solver), no regularization is applied.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      " |  \n",
      " |  dual : bool, optional (default=False)\n",
      " |      Dual or primal formulation. Dual formulation is only implemented for\n",
      " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, optional (default=1e-4)\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, optional (default=1.0)\n",
      " |      Inverse of regularization strength; must be a positive float.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |  \n",
      " |  fit_intercept : bool, optional (default=True)\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  intercept_scaling : float, optional (default=1)\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', optional (default=None)\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *class_weight='balanced'*\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      The seed of the pseudo random number generator to use when shuffling\n",
      " |      the data.  If int, random_state is the seed used by the random number\n",
      " |      generator; If RandomState instance, random_state is the random number\n",
      " |      generator; If None, the random number generator is the RandomState\n",
      " |      instance used by `np.random`. Used when ``solver`` == 'sag' or\n",
      " |      'liblinear'.\n",
      " |  \n",
      " |  solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},              optional (default='liblinear').\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem.\n",
      " |  \n",
      " |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      " |        'saga' are faster for large ones.\n",
      " |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      " |        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      " |        schemes.\n",
      " |      - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
      " |      - 'liblinear' and 'saga' also handle L1 penalty\n",
      " |      - 'saga' also supports 'elasticnet' penalty\n",
      " |      - 'liblinear' does not handle no penalty\n",
      " |  \n",
      " |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      " |      features with approximately the same scale. You can\n",
      " |      preprocess the data with a scaler from sklearn.preprocessing.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |      .. versionchanged:: 0.20\n",
      " |          Default will change from 'liblinear' to 'lbfgs' in 0.22.\n",
      " |  \n",
      " |  max_iter : int, optional (default=100)\n",
      " |      Maximum number of iterations taken for the solvers to converge.\n",
      " |  \n",
      " |  multi_class : str, {'ovr', 'multinomial', 'auto'}, optional (default='ovr')\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.20\n",
      " |          Default will change from 'ovr' to 'auto' in 0.22.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      " |      number for verbosity.\n",
      " |  \n",
      " |  warm_start : bool, optional (default=False)\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      Number of CPU cores used when parallelizing over classes if\n",
      " |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      " |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      " |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors.\n",
      " |      See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  l1_ratio : float or None, optional (default=None)\n",
      " |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      " |      used if ``penalty='elasticnet'`. Setting ``l1_ratio=0`` is equivalent\n",
      " |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      " |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      " |      combination of L1 and L2.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  classes_ : array, shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : array, shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      " |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      " |  \n",
      " |  intercept_ : array, shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      " |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      " |      outcome 0 (False).\n",
      " |  \n",
      " |  n_iter_ : array, shape (n_classes,) or (1, )\n",
      " |      Actual number of iterations for all classes. If binary or multinomial,\n",
      " |      it returns only 1 element. For liblinear solver, only the maximum\n",
      " |      number of iteration across all classes is given.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |  \n",
      " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
      " |  ...                          multi_class='multinomial').fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :]) # doctest: +ELLIPSIS\n",
      " |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      " |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.97...\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  SGDClassifier : incrementally trained logistic regression (when given\n",
      " |      the parameter ``loss=\"log\"``).\n",
      " |  LogisticRegressionCV : Logistic regression with built-in cross validation\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon,\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller tol parameter.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  LIBLINEAR -- A Library for Large Linear Classification\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      " |  \n",
      " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      " |      https://hal.inria.fr/hal-00860051/document\n",
      " |  \n",
      " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      " |      SAGA: A Fast Incremental Gradient Method With Support\n",
      " |      for Non-Strongly Convex Composite Objectives\n",
      " |      https://arxiv.org/abs/1407.0202\n",
      " |  \n",
      " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      " |      methods for logistic regression and maximum entropy models.\n",
      " |      Machine Learning 85(1-2):41-75.\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegression\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.linear_model.base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model.base.SparseCoefMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='warn', max_iter=100, multi_class='warn', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,) optional\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             *sample_weight* support to LogisticRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Log of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is the signed distance of that\n",
      " |      sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "help(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
